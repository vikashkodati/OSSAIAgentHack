{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Llama 3.1 Rag Agent with LlamaIndex\n",
    "\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/ytang07/ai_agents_cookbooks/blob/main/llamaindex/llama31_8b_rag_agent.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>\n",
    "\n",
    "This notebook will walk you through building a LlamaIndex ReactAgent using Llama 3.1 70b. We will be using [OctoAI](https://octo.ai) as our embeddings and llm provider.\n",
    "\n",
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install -qU llama-index llama-index-llms-openai llama-index-readers-file octoai llama-index-llms-octoai llama-index-embeddings-octoai llama-index-embeddings-openai llama-index-llms-openai-like\n",
    "\n",
    "# ! pip freeze | grep llama-index-core\n",
    "# ! pip freeze | grep embeddings-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup API Keys\n",
    "To run the rest of the notebook you will need access to an OctoAI API key. You can sign up for an account [here](https://octoai.cloud/). If you need further guidance you can check OctoAI's [documentation page](https://octo.ai/docs/getting-started/how-to-create-octoai-access-token)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "# from getpass import getpass\n",
    "# environ[\"OCTOAI_API_KEY\"] = getpass(\"Input your OCTOAI API key: \")\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OCTOAI_API_KEY = environ[\"OCTOAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and setup LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    StorageContext,\n",
    "    load_index_from_storage,\n",
    ")\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "from llama_index.embeddings.octoai import OctoAIEmbedding\n",
    "from llama_index.core import Settings as LlamaGlobalSettings\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "# Set the default model to use for embeddings\n",
    "LlamaGlobalSettings.embed_model = OctoAIEmbedding()\n",
    "\n",
    "# Create an llm object to use for the QueryEngine and the ReActAgent\n",
    "llm = OpenAILike(\n",
    "    model=\"meta-llama-3.1-70b-instruct\",\n",
    "    api_base=\"https://text.octoai.run/v1\",\n",
    "    api_key=environ[\"OCTOAI_API_KEY\"],\n",
    "    context_window=40000,\n",
    "    is_function_calling_model=True,\n",
    "    is_chat_model=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"./storage/luma\"\n",
    "    )\n",
    "    luma_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(\n",
    "        persist_dir=\"./storage/profiles\"\n",
    "    )\n",
    "    profile_index = load_index_from_storage(storage_context)\n",
    "\n",
    "    index_loaded = True\n",
    "except:\n",
    "    index_loaded = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the point we create our vector indexes, by calculating the embedding vectors for each of the chunks. You only need to run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4909bcbba22f4bafb20d5f68ccfd734d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e3162102404cc9b1dd8040b6cf1dcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bd8be8fba0420e94ecf7d0efbc3b0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78cb9fcee10a47f296c46a63f0d429f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not index_loaded:\n",
    "    # load data\n",
    "    luma_docs = SimpleDirectoryReader(\n",
    "        input_files=[\"./luma.json\"]\n",
    "    ).load_data()\n",
    "    profile_docs = SimpleDirectoryReader(\n",
    "        input_files=[\"./profiles/vikash.pdf\"]\n",
    "    ).load_data()\n",
    "    \n",
    "    # build index\n",
    "    luma_index = VectorStoreIndex.from_documents(luma_docs, show_progress=True)\n",
    "    profile_index = VectorStoreIndex.from_documents(profile_docs, show_progress=True)\n",
    "\n",
    "    # persist index\n",
    "    luma_index.storage_context.persist(persist_dir=\"./storage/luma\")\n",
    "    profile_index.storage_context.persist(persist_dir=\"./storage/profiles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the query engines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "luma_engine = luma_index.as_query_engine(similarity_top_k=3, llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the query engines as tools that will be used by the agent.\n",
    "\n",
    "As there is a query engine per document we need to also define one tool for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=luma_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"luma_10k\",\n",
    "            description=(\n",
    "                \"Provides information about Luma events. \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=luma_engine,\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"profile_10k\",\n",
    "            description=(\n",
    "                \"Provides information about attendee profiles \"\n",
    "                \"Use a detailed plain text question as input to the tool.\"\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Agent\n",
    "Now we have all the elements to create a LlamaIndex ReactAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ReActAgent.from_tools(\n",
    "    query_engine_tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_turns=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can interact with the agent and ask a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step ffcd1d1b-ffe0-400e-9ccf-144a746f1811. Step input: Which luma events are upcoming? and which event matches the profile?\n",
      "\u001b[1;3;38;5;200mThought: The current language of the user is: English. I need to use a tool to help me answer the question about upcoming Luma events.\n",
      "Action: luma_10k\n",
      "Action Input: {'input': 'upcoming Luma events'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: There are several upcoming events. \n",
      "\n",
      "1. Community Picnic for WAC Seattle's First Birthday on August 29th at Green Lake Park.\n",
      "2. Seattle: Epic Launch Party (Design Buddies & UXGO) on August 29th.\n",
      "3. Crowdsource Choir - \"Sing it Home\" In the Turbine Cathedral on August 29th at Georgetown Steam Plant.\n",
      "4. Lynnwood Link Celebration Social on August 29th at Hemlock State Brewing.\n",
      "5. Lecture 5: Fundraising Opportunities on September 5th.\n",
      "6. Walky Talky with the Author: The Empathy Dilemma on September 12th at The Collective Seattle.\n",
      "7. Club Cascadia Book Club on September 13th at Green Lake Park.\n",
      "8. LGBTQ+ Founders and Funders Meetup on September 18th at Venture Mechanics Startup Launchpad.\n",
      "9. SEA NET: 1st Edition on September 22nd.\n",
      "\u001b[0m> Running step 00b5b85c-20c0-4d81-b59f-32b9007623e7. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: Now that I have the list of upcoming Luma events, I need to use a tool to help me answer the question about which event matches the profile.\n",
      "Action: profile_10k\n",
      "Action Input: {'input': 'profile matching the upcoming Luma events'}\n",
      "\u001b[0m\u001b[1;3;34mObservation: Based on the provided information, the following profiles can be matched with the upcoming Luma events:\n",
      "\n",
      "1. **Healthcare professionals**: \n",
      "   - Seattle Meetup with Health Tech Nerds (August 28th)\n",
      "   - Community Picnic for WAC Seattle's First Birthday (August 29th)\n",
      "\n",
      "2. **Design enthusiasts and creatives**: \n",
      "   - Seattle: Epic Launch Party (Design Buddies & UXGO) (August 29th)\n",
      "\n",
      "3. **Music lovers**: \n",
      "   - Crowdsource Choir - \"Sing it Home\" In the Turbine Cathedral (August 29th)\n",
      "\n",
      "4. **Biotech professionals and entrepreneurs**: \n",
      "   - Lecture 5: Fundraising Opportunities (September 5th)\n",
      "\n",
      "5. **Empathy and leadership enthusiasts**: \n",
      "   - Walky Talky with the Author: The Empathy Dilemma (September 12th)\n",
      "\n",
      "6. **Open-source AI enthusiasts and developers**: \n",
      "   - Open Source AI Hackathon #8 (August 24th)\n",
      "\n",
      "7. **Fitness enthusiasts**: \n",
      "   - Crunch & Brunch x Inspire Seattle (August 24th)\n",
      "\n",
      "8. **Entrepreneurs, tech professionals, investors, and operators**: \n",
      "   - Good Vibes Social Club Presents: Social #2 (August 28th)\n",
      "\n",
      "9. **Community builders and supporters**: \n",
      "   - Celebrate Lynnwood Link (August 29th)\n",
      "\u001b[0m> Running step 7f8d8fd0-bcbf-480f-95be-79b46e820aa6. Step input: None\n",
      "\u001b[1;3;38;5;200mThought: I can answer without using any more tools. I'll use the user's language to answer\n",
      "Answer: The upcoming Luma events are: \n",
      "\n",
      "1. Community Picnic for WAC Seattle's First Birthday on August 29th at Green Lake Park.\n",
      "2. Seattle: Epic Launch Party (Design Buddies & UXGO) on August 29th.\n",
      "3. Crowdsource Choir - \"Sing it Home\" In the Turbine Cathedral on August 29th at Georgetown Steam Plant.\n",
      "4. Lynnwood Link Celebration Social on August 29th at Hemlock State Brewing.\n",
      "5. Lecture 5: Fundraising Opportunities on September 5th.\n",
      "6. Walky Talky with the Author: The Empathy Dilemma on September 12th at The Collective Seattle.\n",
      "7. Club Cascadia Book Club on September 13th at Green Lake Park.\n",
      "8. LGBTQ+ Founders and Funders Meetup on September 18th at Venture Mechanics Startup Launchpad.\n",
      "9. SEA NET: 1st Edition on September 22nd.\n",
      "\n",
      "The event that matches the profile is dependent on the individual's interests. For example, healthcare professionals may be interested in the Community Picnic for WAC Seattle's First Birthday, while design enthusiasts and creatives may be interested in the Seattle: Epic Launch Party (Design Buddies & UXGO).\n",
      "\u001b[0mThe upcoming Luma events are: \n",
      "\n",
      "1. Community Picnic for WAC Seattle's First Birthday on August 29th at Green Lake Park.\n",
      "2. Seattle: Epic Launch Party (Design Buddies & UXGO) on August 29th.\n",
      "3. Crowdsource Choir - \"Sing it Home\" In the Turbine Cathedral on August 29th at Georgetown Steam Plant.\n",
      "4. Lynnwood Link Celebration Social on August 29th at Hemlock State Brewing.\n",
      "5. Lecture 5: Fundraising Opportunities on September 5th.\n",
      "6. Walky Talky with the Author: The Empathy Dilemma on September 12th at The Collective Seattle.\n",
      "7. Club Cascadia Book Club on September 13th at Green Lake Park.\n",
      "8. LGBTQ+ Founders and Funders Meetup on September 18th at Venture Mechanics Startup Launchpad.\n",
      "9. SEA NET: 1st Edition on September 22nd.\n",
      "\n",
      "The event that matches the profile is dependent on the individual's interests. For example, healthcare professionals may be interested in the Community Picnic for WAC Seattle's First Birthday, while design enthusiasts and creatives may be interested in the Seattle: Epic Launch Party (Design Buddies & UXGO).\n"
     ]
    }
   ],
   "source": [
    "response = agent.chat(\"Which luma events are upcoming? and which event matches the profile?\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
